{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "__all_posts: List[dict] = []\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "def read_data_from_remote_resource(url):\n",
    "    return BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "\n",
    "def load_available_pages(habr_all_page):\n",
    "    def find_pagination_elements(html_page):\n",
    "        return list(map(lambda x: x.find_all('a'), html_page.find_all('ul', id='nav-pagess')))[0]\n",
    "    def map_pagination_elements_to_pages(elements_list):\n",
    "        return list(map(lambda element: element.attrs.get('href'), elements_list))\n",
    "    return map_pagination_elements_to_pages(find_pagination_elements(habr_all_page))\n",
    "\n",
    "def find_last_page_num(available_pages_list):\n",
    "    return int(list(map(lambda x: re.findall('\\d+', x), available_pages_list))[len(available_pages_list) - 1][0])\n",
    "\n",
    "def extract_posts_from_page_content(html_page):\n",
    "    def extract_posts_content_block(habr_all_posts_page):\n",
    "        return habr_all_posts_page.find_all('div', class_='posts_list')\n",
    "    def extract_only_posts(habr_posts_block):\n",
    "        return list(map(lambda element: element.find_all('article', class_='post_preview'), habr_posts_block))[0]\n",
    "    return extract_only_posts(extract_posts_content_block(html_page))\n",
    "\n",
    "def get_post_content(post: BeautifulSoup):\n",
    "    def get_title(post_data: BeautifulSoup):\n",
    "        return post_data.find_all('a', class_='post__title_link')[0].text\n",
    "    def get_content(post_data: BeautifulSoup):\n",
    "        return post_data.find_all('div', class_='post__text')[0].text\n",
    "    def get_link(post_data: BeautifulSoup):\n",
    "        return post_data.find_all('a', class_='post__title_link')[0].attrs.get('href')\n",
    "    return {\n",
    "        'title': get_title(post),\n",
    "        'content': get_content(post),\n",
    "        'link': get_link(post)\n",
    "    }\n",
    "\n",
    "def _is_contains_keywods(post_dict: dict):\n",
    "    for keyword in KEYWORDS:\n",
    "        if keyword.lower() in str(post_dict.get('title')).lower() \\\n",
    "                or keyword.lower() in str(post_dict.get('content')).lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def add_posts_from_collection(posts_collection: list):\n",
    "    for post in posts_collection:\n",
    "        _add_post(post)\n",
    "\n",
    "def _add_post(post: BeautifulSoup):\n",
    "    post_dict = get_post_content(post)\n",
    "    if _is_contains_keywods(post_dict):\n",
    "        __all_posts.append(post_dict)\n",
    "\n",
    "first_page = read_data_from_remote_resource('https://habr.com/ru/all/')\n",
    "max_page_number = find_last_page_num(load_available_pages(first_page))\n",
    "\n",
    "add_posts_from_collection(extract_posts_from_page_content(first_page))\n",
    "\n",
    "for i in range(2, max_page_number + 1):\n",
    "    page = read_data_from_remote_resource('https://habr.com/ru/all/page' + str(i) + '/')\n",
    "    add_posts_from_collection(extract_posts_from_page_content(page))\n",
    "\n",
    "pd.DataFrame(__all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "URL = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "emails = ['barkal97@mail.ru','barkal4ik@gmail.com']\n",
    "\n",
    "for email in emails:\n",
    "    params = {'emailAddresses': [email]}\n",
    "    res = requests.post(URL, data = json.dumps(params), headers = {'Vaar-Version': '0', 'Vaar-Header-App-Product': 'hackcheck-web-avast'})\n",
    "    time.sleep(0.3)\n",
    "    res.json()\n",
    "    table = pd.DataFrame(res.json()['breaches'])\n",
    "    transpon_table =  table.transponse()\n",
    "    table_1 = transpon_table.loc[:,['publishDate', 'site', 'description']].head()\n",
    "table_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
